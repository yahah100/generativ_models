{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exercise05_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb8PqzDyujYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.utils as vutils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions.normal import Normal\n",
        "import time\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot  as plt\n",
        "import matplotlib.animation as animation\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.display import HTML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS-J336uv0eT",
        "colab_type": "text"
      },
      "source": [
        "# Generativ Adversarial Networks\n",
        "In this problem, you will investigate different topics related to GANâ€™s.\n",
        "## (a) Part A\n",
        "Model Design\n",
        "- Design a DCGAN of your own choice for the colored MNIST. you can get help\n",
        "from https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
        "You can access the colored MNIST using the information from the previous\n",
        "exercise. You can find helpful hints for training from https://medium.com/@jonathan_hui/gan-gan-series-2d279f906e7b\n",
        "Deliverables\n",
        "- summerize your configuration and techniques used for training\n",
        "- Having your model trained, generate 32 examples from the generator. Summerize\n",
        "frequency of each digit generated by the generator in a table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS0yIrHuvy8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 100\n",
        "\n",
        "# cpu or gpu\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Number Gpus\n",
        "ngpu = 1\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this\n",
        "#   size using a transformer.\n",
        "image_size = 64\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = 3\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 100\n",
        "\n",
        "# Size of feature maps in generator\n",
        "ngf = 16\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 16\n",
        "\n",
        "# Number of training epochs\n",
        "EPOCHS = 5\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 0.0002\n",
        "\n",
        "# Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFzlJ7Unu_tU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b4561046-399d-4da6-f92f-9431a3bd69b4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oepymGaVvAG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('drive/My Drive/Colab Notebooks/colored-mnist.pkl', 'rb') as f:\n",
        "  data = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDpDfQOtvAI1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5ea30d70-3012-4c60-9098-aeac7815370e"
      },
      "source": [
        "train = data['train']\n",
        "test = data['test']\n",
        "\n",
        "print(\"Count train samples: \", train.shape[0])\n",
        "print(\"Count test samples: \", test.shape[0])\n",
        "print(\"Shape of image (H, W, C): \", train[0].shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count train samples:  60000\n",
            "Count test samples:  10000\n",
            "Shape of image (H, W, C):  (28, 28, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S08DXDEavANC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "dcbb7385-50df-4384-aa7d-4f1553798164"
      },
      "source": [
        "fig = plt.figure(figsize=(10, 10))\n",
        "def show_image(i, image): \n",
        "  print_image =  image * (255 / 3)\n",
        "  fig.add_subplot(1, 4, i+1)\n",
        "  plt.imshow(print_image.astype(int))\n",
        "  \n",
        "\n",
        "for i in range(0, 4):\n",
        "  show_image(i, train[i])\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAACaCAYAAABmDna+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAT8UlEQVR4nO3db6gl9X3H8c+31jyJXljrZVmMdMMi\nBZ9Uy0VaEkpKajE+MXmwEh+ELQibBxEUfJBN+iB5uIHEPEkJbFDWgLXsokEp0tSIIIEiXkXMqhht\n2BBlda9k4UgflJp++2DPxnPnnHvn929mfjPn/YLL3nvuOWe+c+Zz5v525nt+Y+4uAAAAhPuToQsA\nAAAYGwZQAAAAkRhAAQAARGIABQAAEIkBFAAAQCQGUAAAAJGyBlBmdoeZvWVm75jZiVJFYb2QI+Qi\nQyiBHCGGpc4DZWZXSfq1pNslvSvpJUn3uPsbez3m+o0NP7y5mbS8TxwIuM+lzGXkLDtFs96ulhMi\n77U7v7OjD2czC71/fzlqvqZdZSRk2WOS+jrlrfP5nfP6cPZhUI7KZGjI/QpSrNwaCzfu/PeOZv9T\n474IIULebSl7maXnbVnQfjn604TlX3GbpHfc/TeSZGb/KukuSXuG7fDmprZPnsxYpCQdDbjP2cxl\n5Cw7RbPerpYTIu+12zoR/Z+2nnLUfE27ykjIssck9XXKW+etE1sxdy+QoSH3K0ixcmss3HjiF7Xu\nixAi5N2WspdZet6WBe2Xo5xTeDdI+t3Cz+/ObwNikCPkIkMogRwhSudN5GZ23My2zWx7ZzbrenGY\nKHKEXGQIJZAjXJFzCu89STcu/PyZ+W27uPspSackaevIkQIX3lt1vG3Mp0ikbuofzemGnnI0ttO6\nTUOd5l21nJBa2l7vovUXyFDIfmVqp/nGvT4pqWsRnaMjR454btJ7O1WV+DwpampK6VLOEaiXJN1k\nZp81s09J+qqkp8uUhTVCjpCLDKEEcoQoyUeg3P1jM7tP0s8lXSXpEXd/vVhlWAvkCLnIEEogR4iV\ncwpP7v6MpGcK1YI1RY6QiwyhBHKEGMxEDgAAECnrCFSsSwppLU1puau30bHu5l/ESX1NS2zzlHms\nhmzdnGrbaJtS7b4Yo5St39U7paZ3f017q5IzAnIECgAAIBIDKAAAgEgMoAAAACL12gN1QDV1RgzZ\niwTkqqnDAXn6uk5jiQ4d+rUW1fU3LV9XvUo190Tl4AgUAABAJAZQAAAAkRhAAQAAROq1B2r5jHGp\nOZ04T99c5ylecrlOw7yqdtY6eV4/WuB638jEO3U8Qv6mDSPlb0Cp5LXNfjiVhHMECgAAIBIDKAAA\ngEgMoAAAACJl9UCZ2XlJH0n6g6SP3X2rRFFYL+QIJZAj5CJDiFGiifzv3P3DsLuGXE44Rcpz5l/k\nt6tG3jBH9/lptbsD7uNHB2vvi8jR+lnOGhet3UNgjqY2BWJX4jNjZ+Mfk7LfSVhMqIx9Uch6xBc+\ntokn22pJycgqA/69ksQpPAAAgGi5AyiX9B9m9rKZHV91BzM7bmbbZra9M5tlLg4TRY5Qwr452p2h\nnQHKwwhE7ovI0TrLHUB93t3/StKXJH3DzP62eQd3P+XuW+6+tbmxkbk4TBQ5Qgn75mh3hjaHqRC1\ni9wXkaN1ltUD5e7vzf+9aGY/k3SbpBf2un+zAyrs7GU355SXz8GGdAjVa8zdLbE5Gk5/F3lN63nK\nry+kr6/WyTb7ztGYX6tw/eSuFtEZCmmlK/DylLiA75BK9TzVJvkIlJl92syuvfK9pH+QdK5UYVgP\n5AglkCPkIkOIlXME6qCkn5nZlef5F3f/9yJVYZ2QI5RAjpCLDCFK8gDK3X8j6S8L1oI1RI5QAjlC\nLjKEWL1eTHj5dPGqs7TNc6Ul+jpSZ9Go+bxtW23tZ8CHnkNjXZXpv6s5m1gnffW3TLSNBj1bilHG\nn37mgQIAAIjEAAoAACASAygAAIBIDKAAAAAi9dpEvqymrsCaatmtu8n46l3n8ShxUd9+PkxRv8V1\nvDRYFViUktWm5ceEfIBlFE3jzdmhgzTXfQwrGmdUE2dmlMoRKAAAgEgMoAAAACIxgAIAAIg0cA9U\nP1adbx/qHO2qfqaQi5J2Y70uFDqUtAsDT79PYtniOn+v42WlXdp8/fC67I8cdfW3tKuJnkvuWTkC\nBQAAEIkBFAAAQCQGUAAAAJFaB1Bm9oiZXTSzcwu3XWdmz5rZ2/N/D3RbJsaOHKEEcoRcZAilhDSR\nn5b0I0k/XbjthKTn3P2kmZ2Y//zNticKa7frp3m22aDWXSNc+ySY3U2UWUKxRvPTKpSj/sSv++oc\ndTXZZttjdvOjZ1qfYbn+5edsNsX3nN/TKpKjA4rdLsN92KMmpfI9qNMqti+Kz9F6fCAkXldN411q\nPQLl7i9I+n3j5rskPTr//lFJXy5cFyaGHKEEcoRcZAilpPZAHXT3C/Pv35d0cK87mtlxM9s2s+3Z\nbJa4OExUUo52yBF2C8rR7gzt9FcdxiBxX0SO1ll2E7m7u6Q9j+G7+yl333L3rY2NjdzFYaJicrRJ\njrCH/XK0O0ObPVeGsYjbF5GjdZY6keYHZnbI3S+Y2SFJF0MeFHa2eKgJLlf1hzT7pOJ7IAbuF+lJ\n8gSISTnaf/lXdJGjVf1AXeV19zot9weUuajrsrsLLKd3hXKEZQE9fm2xConL8JHqMEMpK1fvRLqj\nulDwCiWrTz0C9bSkY/Pvj0l6qkw5WDPkCCWQI+QiQ4gWMo3B45L+U9JfmNm7ZnavpJOSbjeztyX9\n/fxnYE/kCCWQI+QiQyil9RSeu9+zx6++WLgWTBg5QgnkCLnIEErp9WLCzXmgVhnfTBBNKQ0B41/r\nYcWf1Q7bCiFzPK2jrvK6+Ppe6mgZQ5vg+7/lbbFqep/1eiul9DPV8wJ1sd/rc86npeoLrg6XcgEA\nAIjEAAoAACASAygAAIBIDKAAAAAi9dpEnnLZxZo0J9tMaa5b/Zj9L+A6zck304VdlHp/Q+awTANl\nSjNyqceUeN6p6qthOP/1DZsYOGE5jYccPTvlLITsjeppCF9HS+/IgnOUcgQKAAAgEgMoAACASAyg\nAAAAIvXaA5XWBTXk+eP9l72ql6W9Lyqlb2q5V6G9L4q+lFr0OWlcvGZOQmpN6esJed7ki1IXELJO\nXW3H/IuWpyyn1PpUHe8q1Hth4KauJguuaR/Y2hMlBW8ijkABAABEYgAFAAAQqXUAZWaPmNlFMzu3\ncNt3zew9M3t1/nVnt2Vi7MgRcpEhlECOUEpID9RpST+S9NPG7T909+/nLb7ec8Gpmud67ezdjXuU\n6TFp9kks90Slntjt7Fz1aXWUo666ve6u+oqnKds35FVJyWtvc9+cVrEMhVzavKnUvG9DKdHv1v6Y\nEcz7dFq9/k1L6T3rJzdl8rn8HMt/j2p6H+yWs/dqPQLl7i9I+n1URUADOUIuMoQSyBFKyemBus/M\nXpsfDj1QrCKsG3KEXGQIJZAjREkdQP1Y0hFJt0i6IOkHe93RzI6b2baZbe/MdhIXh4lKytFsNuur\nPtQvcV9EhrALOUK0pAGUu3/g7n9w9/+T9BNJt+1z31PuvuXuW5sbm6l1YoJSc7SxsdFfkaha+r6I\nDOET5AgpkibSNLND7n5h/uNXJJ3b7/6f2N24OWhLc0+azXTLTeV9quvVTM1RyHSsKe2ybc8xdt1N\n0LhKP1lL3xfFC7uY+DCpSbvgeMiHVQKepXGfM83fhxc0mD5zFIY91hi0DqDM7HFJX5B0vZm9K+k7\nkr5gZrdIcknnJX29wxoxAeQIucgQSiBHKKV1AOXu96y4+eEOasGEkSPkIkMogRyhFGYiBwAAiNTr\nxYRTpq6bmmYfxSopvSrtE2siXnwnVX8XzWyfsG85RylTxq2aJK89w+3PU1c/Xsprs3pbt63XkOud\nsPcdz3VwEbCxupg4M+xvzTSDxBEoAACASAygAAAAIjGAAgAAiNRrD1TI/D1T0+/cO21SzjuPc4uV\nqbqmdQ/pb8i/EHBaD1dKrtoecynhOcegq0tfhy4r7iFLcQgotarrJ6+1VRtryHkIp4cjUAAAAJEY\nQAEAAERiAAUAABCJARQAAECkXpvIuzLUFH1hk5L10yC6PJnZdC/VvJ4Tsu5e4zIT4vU58WfthkzU\nQB/uWLHY9Xtf5Vq/j0Z1deHqoeTsSjkCBQAAEIkBFAAAQKTWAZSZ3Whmz5vZG2b2upndP7/9OjN7\n1szenv97oPtyMVbkCLnIEEogRyglpAfqY0kPuvsrZnatpJfN7FlJ/yjpOXc/aWYnJJ2Q9M39nqjZ\nu1LqzHFXZ6BL9Zl0Ie089KDn6ovlqGY1ZyZNSGZ6W+e1yFCYqeWsV2uRo7omca5qP1JM6xEod7/g\n7q/Mv/9I0puSbpB0l6RH53d7VNKXuyoS40eOkIsMoQRyhFKieqDM7LCkWyW9KOmgu1+Y/+p9SQf3\neMxxM9s2s+3ZbJZRKqaCHCFXboZ2yBBUIkc7vdSJOgUPoMzsGklPSHrA3XftfdzdJa08p+Tup9x9\ny923NjY2sorF+JEj5CqRoU0ytPbK5Gizh0pRq6B5oMzsal0O2mPu/uT85g/M7JC7XzCzQ5IudlVk\nF4brVYlfrh89E/A8CVf97FntOarvFctTZo6nPi9+2667DK1ap7b3au19HS31HV3fiaBq3xelqKvn\nqV4l//SHfArPJD0s6U13f2jhV09LOjb//pikp8qVhakhR8hFhlACOUIpIUegPifpa5J+ZWavzm/7\ntqSTks6Y2b2Sfivp7m5KxESQI+QiQyiBHKGI1gGUu/9S0l7HBr9YthxMFTlCLjKEEsgRSmEmcgAA\ngEjVX0w45ZK4Y5vMsH1SzJD1GeqSytPR9grWlKphLwJc0ytRSso6TfF1wDpLm6A5RMjfp7Z9Wtr7\nrcvhAEegAAAAIjGAAgAAiMQACgAAIFL1PVAp/U3D9q7sXtryOeVV1ZSoMGWtu5gI8FLk/et1tNFn\ndHbA3rphe54wvJT+kPi8NpfS/Bw/KWy6pN2vM6/QatN8XTgCBQAAEIkBFAAAQCQGUAAAAJEYQAEA\nAETqtYn8gGpqJcufeHJ1Y++ZluV0JaVpvFSj+aLvRd6/Xs01P1Ookbue9wCmI+EDISve/kzHG6uu\nv2rjMY1JaDkCBQAAEIkBFAAAQKTWAZSZ3Whmz5vZG2b2upndP7/9u2b2npm9Ov+6s/tyMVbkCLnI\nEEogRyglpAfqY0kPuvsrZnatpJfN7Nn5737o7t/vrrxlIRMKNs+uNruSLos/bx121nYa53Y7UFWO\nQnS1JVP6TEIek1JvSvfGgH0yo8tQOR2kccWGO3q2eZfmZLLlyxjAJHPU3YWASwgITttdupj3OVPr\nAMrdL0i6MP/+IzN7U9INXReGaSFHyEWGUAI5QilRPVBmdljSrZJenN90n5m9ZmaPmNmBPR5z3My2\nzWx7ZzbLKhbTkJujGTlae+yLUEJ+jnZ6qhQ1Ch5Amdk1kp6Q9IC7zyT9WNIRSbfo8mj+B6se5+6n\n3H3L3bc2NzYKlIwxK5GjDXK01tgXoYQyOdrsrV7UJ2geKDO7WpeD9pi7PylJ7v7Bwu9/Iunf2p6n\nednFVUr0U4TMdpSynPHP9tFV902YUjnqS18XpU553i5m8Ao15PtgbBlKk9LhFvCYkLfy0m27bzja\nbJLSOPuixp+jUn/VKtJSfo05C/kUnkl6WNKb7v7Qwu2HFu72FUnnypeHqSBHyEWGUAI5QikhR6A+\nJ+lrkn5lZq/Ob/u2pHvM7BZJLum8pK93UiGmghwhFxlCCeQIRYR8Cu+XkmzFr54pXw6mihwhFxlC\nCeQIpTATOQAAQKReLyaMKWjr5LvUSxV9qLBnMQoXhh2rUh8RaNxnRQN4eym7HzP298R4dPXRkqaK\n9wojCBtHoAAAACIxgAIAAIjEAAoAACCSufd3AUIz25H0W0nXS/qwtwXnGVOt0vD1/rm7dzo9Lznq\nXA21dpqjkWZIGle9Q9fKvmg1ao2zZ456HUD9caFm2+6+1fuCE4ypVml89eYY07pSa53Gtq5jqndM\nteYa07pSazmcwgMAAIjEAAoAACDSUAOoUwMtN8WYapXGV2+OMa0rtdZpbOs6pnrHVGuuMa0rtRYy\nSA8UAADAmHEKDwAAIFLvAygzu8PM3jKzd8zsRN/L34+ZPWJmF83s3MJt15nZs2b29vzfA0PWeIWZ\n3Whmz5vZG2b2upndP7+9ynpLqjlDEjkai5pzRIbGoeYMSeSoa70OoMzsKkn/LOlLkm6WdI+Z3dxn\nDS1OS7qjcdsJSc+5+02Snpv/XIOPJT3o7jdL+mtJ35i/lrXWW8QIMiSRo+qNIEenRYaqNoIMSeSo\nW+7e25ekv5H084WfvyXpW33WEFDjYUnnFn5+S9Kh+feHJL01dI171P2UpNvHUm/GelafoXld5Kji\nrzHkiAzV/TWGDM3rIkcdffV9Cu8GSb9b+Pnd+W01O+juF+bfvy/p4JDFrGJmhyXdKulFjaDeTGPM\nkDSC7UKOqs9R9duEDFWfIWkE22UsOaKJPIJfHgJX9bFFM7tG0hOSHnD32eLvaqwXdW4XcjQuNW4T\nMjQ+NW6XMeWo7wHUe5JuXPj5M/PbavaBmR2SpPm/Fweu54/M7GpdDtpj7v7k/OZq6y1kjBmSKt4u\n5EjSOHJU7TYhQ5LGkSGp4u0ythz1PYB6SdJNZvZZM/uUpK9KerrnGmI9LenY/PtjunxednBmZpIe\nlvSmuz+08Ksq6y1ojBmSKt0u5GhUOapym5ChUWVIqnS7jDJHAzSG3Snp15L+S9I/Dd0E1qjtcUkX\nJP2vLp/PvlfSn+ly5//bkn4h6bqh65zX+nldPpT5mqRX51931lrvumSIHI3nq+YckaFxfNWcIXLU\n/RczkQMAAESiiRwAACASAygAAIBIDKAAAAAiMYACAACIxAAKAAAgEgMoAACASAygAAAAIjGAAgAA\niPT/brNIOkF4TJUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTmjW4B3vAOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, target, transform=None):\n",
        "    self.target =  torch.from_numpy(target).permute(0, 3, 1, 2)\n",
        "    self.transform = transform\n",
        "      \n",
        "  def __getitem__(self, index):\n",
        "    x = self.target[index]\n",
        "    \n",
        "    if self.transform:\n",
        "        x = self.transform(x)\n",
        "        \n",
        "    return x\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imIyw_M2vntI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose([ transforms.ToPILImage(), transforms.Resize(size=64), transforms.ToTensor()])\n",
        "train_ds = MyDataset(train, transform=transform)\n",
        "test_ds = MyDataset(test, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size)\n",
        "testloader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJIX7QS9vnw7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "25be70e1-c335-4092-eb67-e052e1601c34"
      },
      "source": [
        "dataiter = iter(trainloader)\n",
        "image = dataiter.next()\n",
        "print(\"Tensor shape: \", image.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor shape:  torch.Size([100, 3, 64, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0JidaKNwgMp",
        "colab_type": "text"
      },
      "source": [
        "### Define the GAN\n",
        "I used the model from the pytorch Tutorial\n",
        "(https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FrKFz4Ox5WU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "  classname = m.__class__.__name__\n",
        "  if classname.find('Conv') != -1:\n",
        "    nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "  elif classname.find('BatchNorm') != -1:\n",
        "    nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "    nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1wp6-6Nvny5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generator Code\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, ngpu):\n",
        "    super(Generator, self).__init__()\n",
        "    self.ngpu = ngpu\n",
        "    self.main = nn.Sequential(\n",
        "        # input is Z, going into a convolution\n",
        "        nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "        nn.BatchNorm2d(ngf * 8),\n",
        "        nn.ReLU(True),\n",
        "        # state size. (ngf*8) x 4 x 4\n",
        "        nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(ngf * 4),\n",
        "        nn.ReLU(True),\n",
        "        # state size. (ngf*4) x 8 x 8\n",
        "        nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(ngf * 2),\n",
        "        nn.ReLU(True),\n",
        "        # state size. (ngf*2) x 16 x 16\n",
        "        nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(ngf),\n",
        "        nn.ReLU(True),\n",
        "        # state size. (ngf) x 32 x 32\n",
        "        nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
        "        nn.Tanh()\n",
        "        # state size. (nc) x 64 x 64\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "    out = self.main(input)\n",
        "    # print(\"Ge out:\", out.shape)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDJfjt9cwNKD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "0697ce86-efcb-4577-de79-c2022a41f91c"
      },
      "source": [
        "# Create the generator\n",
        "netG = Generator(ngpu).to(device)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
        "\n",
        "# Apply the weights_init function to randomly initialize all weights\n",
        "#  to mean=0, stdev=0.2.\n",
        "netG.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "print(netG)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(100, 128, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (13): Tanh()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCQcaTH5wNOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, ngpu):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.ngpu = ngpu\n",
        "    self.main = nn.Sequential(\n",
        "      # input is (nc) x 64 x 64\n",
        "      nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "      nn.LeakyReLU(0.2, inplace=True),\n",
        "      # state size. (ndf) x 32 x 32\n",
        "      nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "      nn.BatchNorm2d(ndf * 2),\n",
        "      nn.LeakyReLU(0.2, inplace=True),\n",
        "      # state size. (ndf*2) x 16 x 16\n",
        "      nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "      nn.BatchNorm2d(ndf * 4),\n",
        "      nn.LeakyReLU(0.2, inplace=True),\n",
        "      # state size. (ndf*4) x 8 x 8\n",
        "      nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "      nn.BatchNorm2d(ndf * 8),\n",
        "      nn.LeakyReLU(0.2, inplace=True),\n",
        "      # state size. (ndf*8) x 4 x 4\n",
        "      nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "      nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "    out = self.main(input)\n",
        "    # print(\"De out:\", out.shape)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnP07SEuwNIT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "e73f4e2e-9fae-4e9a-fbbf-5cddc1da475d"
      },
      "source": [
        "# Create the Discriminator\n",
        "netD = Discriminator(ngpu).to(device)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
        "    \n",
        "# Apply the weights_init function to randomly initialize all weights\n",
        "#  to mean=0, stdev=0.2.\n",
        "netD.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "print(netD)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (11): Conv2d(128, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (12): Sigmoid()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7xHZAHyvn3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize BCELoss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Create batch of latent vectors that we will use to visualize\n",
        "#  the progression of the generator\n",
        "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usVZFL2828om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_d_net_with_real(data, label):\n",
        "  '''\n",
        "  Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "  '''\n",
        "  ## Train with all-real batch\n",
        "  netD.zero_grad()\n",
        "  # Format batch\n",
        "  real_cpu = data.to(device)\n",
        "\n",
        "  # Forward pass real batch through D\n",
        "  output = netD(real_cpu).view(-1)\n",
        "  # print(\"real_out:\", output.shape)\n",
        "  # Calculate loss on all-real batch\n",
        "  errD_real = criterion(output, label)\n",
        "  # Calculate gradients for D in backward pass\n",
        "  errD_real.backward()\n",
        "  D_x = output.mean().item()\n",
        "  return errD_real, D_x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VXg8n5H3c5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_d_net_with_fake(label, fake, errD_real):\n",
        "  '''\n",
        "  Train with all-fake batch\n",
        "  '''\n",
        "  \n",
        "  label.fill_(fake_label)\n",
        "  # Classify all fake batch with D\n",
        "  output = netD(fake.detach()).view(-1)\n",
        "  # Calculate D's loss on the all-fake batch\n",
        "  errD_fake = criterion(output, label)\n",
        "  # Calculate the gradients for this batch\n",
        "  errD_fake.backward()\n",
        "  D_G_z1 = output.mean().item()\n",
        "  # Add the gradients from the all-real and all-fake batches\n",
        "  errD = errD_real + errD_fake\n",
        "  # Update D\n",
        "  optimizerD.step()\n",
        "  return errD, D_G_z1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxz3jNw24MaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_g_net(data, label, fake):\n",
        "  '''\n",
        "  Update G network: maximize log(D(G(z)))\n",
        "  '''\n",
        "  netG.zero_grad()\n",
        "  label.fill_(real_label)  # fake labels are real for generator cost\n",
        "  # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "  output = netD(fake).view(-1)\n",
        "  # Calculate G's loss based on this output\n",
        "  errG = criterion(output, label)\n",
        "  # Calculate gradients for G\n",
        "  errG.backward()\n",
        "  D_G_z2 = output.mean().item()\n",
        "  # Update G\n",
        "  optimizerG.step()\n",
        "\n",
        "  return errG, D_G_z2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUXkAvojvnqp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        },
        "outputId": "2eefd622-7802-4dd7-a446-09ae68a56c10"
      },
      "source": [
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  netG.train()\n",
        "  netD.train()\n",
        "  for i, x in enumerate(trainloader, 0):\n",
        "    x = x.to(device).float()\n",
        "    \n",
        "    b_size = x.size(0)\n",
        "    label = torch.full((b_size,), real_label, device=device)\n",
        "\n",
        "    errD_real, D_x = train_d_net_with_real(x, label)\n",
        "\n",
        "    # Generate batch of latent vectors\n",
        "    noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "    # Generate fake image batch with G\n",
        "    fake = netG(noise)\n",
        "\n",
        "    errD, D_G_z1 = train_d_net_with_fake(label, fake, errD_real)\n",
        "\n",
        "    errG, D_G_z2 = train_g_net(x, label, fake)\n",
        "\n",
        "\n",
        "    # print stats\n",
        "    if i % 50 == 0:\n",
        "      print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, EPOCHS, i, len(trainloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "    # Save Losses for plotting later\n",
        "    G_losses.append(errG.item())\n",
        "    D_losses.append(errD.item())\n",
        "\n",
        "    # Check how the generator is doing by saving G's output on fixed_noise\n",
        "    if (iters % 500 == 0) or ((epoch == EPOCHS-1) and (i == len(trainloader)-1)):\n",
        "        with torch.no_grad():\n",
        "            fake = netG(fixed_noise).detach().cpu()\n",
        "        img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "    iters += 1\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0/5][0/600]\tLoss_D: 1.2962\tLoss_G: 1.1976\tD(x): 0.5139\tD(G(z)): 0.4361 / 0.3178\n",
            "[0/5][50/600]\tLoss_D: 0.0592\tLoss_G: 5.1539\tD(x): 0.9826\tD(G(z)): 0.0404 / 0.0067\n",
            "[0/5][100/600]\tLoss_D: 0.0183\tLoss_G: 5.9272\tD(x): 0.9960\tD(G(z)): 0.0142 / 0.0031\n",
            "[0/5][150/600]\tLoss_D: 0.0072\tLoss_G: 6.2036\tD(x): 0.9987\tD(G(z)): 0.0059 / 0.0022\n",
            "[0/5][200/600]\tLoss_D: 0.0039\tLoss_G: 6.3122\tD(x): 0.9994\tD(G(z)): 0.0034 / 0.0020\n",
            "[0/5][250/600]\tLoss_D: 0.0027\tLoss_G: 6.4569\tD(x): 0.9998\tD(G(z)): 0.0025 / 0.0017\n",
            "[0/5][300/600]\tLoss_D: 0.0019\tLoss_G: 6.7427\tD(x): 0.9998\tD(G(z)): 0.0017 / 0.0012\n",
            "[0/5][350/600]\tLoss_D: 0.0016\tLoss_G: 6.9085\tD(x): 0.9999\tD(G(z)): 0.0015 / 0.0010\n",
            "[0/5][400/600]\tLoss_D: 0.0015\tLoss_G: 6.8488\tD(x): 0.9999\tD(G(z)): 0.0014 / 0.0011\n",
            "[0/5][450/600]\tLoss_D: 0.0012\tLoss_G: 6.9923\tD(x): 0.9999\tD(G(z)): 0.0012 / 0.0009\n",
            "[0/5][500/600]\tLoss_D: 0.0011\tLoss_G: 7.0700\tD(x): 1.0000\tD(G(z)): 0.0011 / 0.0009\n",
            "[0/5][550/600]\tLoss_D: 0.0010\tLoss_G: 7.2412\tD(x): 0.9999\tD(G(z)): 0.0010 / 0.0007\n",
            "[1/5][0/600]\tLoss_D: 0.0009\tLoss_G: 7.2880\tD(x): 1.0000\tD(G(z)): 0.0009 / 0.0007\n",
            "[1/5][50/600]\tLoss_D: 0.0006\tLoss_G: 7.7816\tD(x): 1.0000\tD(G(z)): 0.0005 / 0.0004\n",
            "[1/5][100/600]\tLoss_D: 0.0007\tLoss_G: 7.5052\tD(x): 0.9999\tD(G(z)): 0.0007 / 0.0006\n",
            "[1/5][150/600]\tLoss_D: 0.0004\tLoss_G: 8.0701\tD(x): 1.0000\tD(G(z)): 0.0004 / 0.0003\n",
            "[1/5][200/600]\tLoss_D: 0.0003\tLoss_G: 8.2350\tD(x): 1.0000\tD(G(z)): 0.0003 / 0.0003\n",
            "[1/5][250/600]\tLoss_D: 0.0005\tLoss_G: 7.9204\tD(x): 1.0000\tD(G(z)): 0.0004 / 0.0004\n",
            "[1/5][300/600]\tLoss_D: 0.0003\tLoss_G: 8.2479\tD(x): 1.0000\tD(G(z)): 0.0003 / 0.0003\n",
            "[1/5][350/600]\tLoss_D: 0.0003\tLoss_G: 8.4126\tD(x): 1.0000\tD(G(z)): 0.0002 / 0.0002\n",
            "[1/5][400/600]\tLoss_D: 0.0002\tLoss_G: 8.7512\tD(x): 1.0000\tD(G(z)): 0.0002 / 0.0002\n",
            "[1/5][450/600]\tLoss_D: 0.0002\tLoss_G: 8.8104\tD(x): 1.0000\tD(G(z)): 0.0002 / 0.0002\n",
            "[1/5][500/600]\tLoss_D: 0.0002\tLoss_G: 8.6102\tD(x): 1.0000\tD(G(z)): 0.0002 / 0.0002\n",
            "[1/5][550/600]\tLoss_D: 0.0002\tLoss_G: 8.6713\tD(x): 1.0000\tD(G(z)): 0.0002 / 0.0002\n",
            "[2/5][0/600]\tLoss_D: 0.0002\tLoss_G: 8.6401\tD(x): 1.0000\tD(G(z)): 0.0002 / 0.0002\n",
            "[2/5][50/600]\tLoss_D: 0.0002\tLoss_G: 8.8748\tD(x): 1.0000\tD(G(z)): 0.0002 / 0.0001\n",
            "[2/5][100/600]\tLoss_D: 0.0002\tLoss_G: 8.8199\tD(x): 1.0000\tD(G(z)): 0.0002 / 0.0001\n",
            "[2/5][150/600]\tLoss_D: 0.0002\tLoss_G: 8.9127\tD(x): 1.0000\tD(G(z)): 0.0001 / 0.0001\n",
            "[2/5][200/600]\tLoss_D: 0.0002\tLoss_G: 8.8104\tD(x): 1.0000\tD(G(z)): 0.0002 / 0.0002\n",
            "[2/5][250/600]\tLoss_D: 0.0002\tLoss_G: 8.6830\tD(x): 1.0000\tD(G(z)): 0.0002 / 0.0002\n",
            "[2/5][300/600]\tLoss_D: 0.0001\tLoss_G: 9.2405\tD(x): 1.0000\tD(G(z)): 0.0001 / 0.0001\n",
            "[2/5][350/600]\tLoss_D: 0.0002\tLoss_G: 8.7130\tD(x): 1.0000\tD(G(z)): 0.0002 / 0.0002\n",
            "[2/5][400/600]\tLoss_D: 0.0001\tLoss_G: 9.2433\tD(x): 1.0000\tD(G(z)): 0.0001 / 0.0001\n",
            "[2/5][450/600]\tLoss_D: 0.0001\tLoss_G: 9.2134\tD(x): 1.0000\tD(G(z)): 0.0001 / 0.0001\n",
            "[2/5][500/600]\tLoss_D: 0.0001\tLoss_G: 9.3233\tD(x): 1.0000\tD(G(z)): 0.0001 / 0.0001\n",
            "[2/5][550/600]\tLoss_D: 0.0001\tLoss_G: 9.3066\tD(x): 1.0000\tD(G(z)): 0.0001 / 0.0001\n",
            "[3/5][0/600]\tLoss_D: 0.0001\tLoss_G: 9.7304\tD(x): 1.0000\tD(G(z)): 0.0001 / 0.0001\n",
            "[3/5][50/600]\tLoss_D: 0.0001\tLoss_G: 9.7920\tD(x): 1.0000\tD(G(z)): 0.0001 / 0.0001\n",
            "[3/5][100/600]\tLoss_D: 0.0001\tLoss_G: 9.5554\tD(x): 1.0000\tD(G(z)): 0.0001 / 0.0001\n",
            "[3/5][150/600]\tLoss_D: 0.0001\tLoss_G: 9.8516\tD(x): 1.0000\tD(G(z)): 0.0001 / 0.0001\n",
            "[3/5][200/600]\tLoss_D: 0.0001\tLoss_G: 9.3052\tD(x): 1.0000\tD(G(z)): 0.0001 / 0.0001\n",
            "[3/5][250/600]\tLoss_D: 0.0000\tLoss_G: 10.6122\tD(x): 1.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[3/5][300/600]\tLoss_D: 0.0001\tLoss_G: 9.6245\tD(x): 1.0000\tD(G(z)): 0.0001 / 0.0001\n",
            "[3/5][350/600]\tLoss_D: 0.0001\tLoss_G: 9.6714\tD(x): 1.0000\tD(G(z)): 0.0001 / 0.0001\n",
            "[3/5][400/600]\tLoss_D: 0.0001\tLoss_G: 9.1445\tD(x): 1.0000\tD(G(z)): 0.0001 / 0.0001\n",
            "[3/5][450/600]\tLoss_D: 0.0001\tLoss_G: 9.5290\tD(x): 1.0000\tD(G(z)): 0.0001 / 0.0001\n",
            "[3/5][500/600]\tLoss_D: 0.0001\tLoss_G: 9.4431\tD(x): 1.0000\tD(G(z)): 0.0001 / 0.0001\n",
            "[3/5][550/600]\tLoss_D: 0.0000\tLoss_G: 10.3512\tD(x): 1.0000\tD(G(z)): 0.0000 / 0.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6TyT2AeC3zC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAdfuqWuxjwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%capture\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
        "\n",
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8IUul60xkOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grab a batch of real images from the dataloader\n",
        "real_batch = next(iter(trainloader))\n",
        "\n",
        "# Plot the real images\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,2,1)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Real Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
        "\n",
        "# Plot the fake images from the last epoch\n",
        "plt.subplot(1,2,2)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Fake Images\")\n",
        "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5VM7iEKCl6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}