{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exercise05_2_part_B",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8Fl0QrSFy4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.utils as vutils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions.normal import Normal\n",
        "import time\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot  as plt\n",
        "import matplotlib.animation as animation\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.display import HTML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuszfurjHe9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 1000\n",
        "\n",
        "# cpu or gpu\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Number Gpus\n",
        "ngpu = 1\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 20\n",
        "\n",
        "# output size\n",
        "n_out = 2\n",
        "\n",
        "# Number of training epochs\n",
        "EPOCHS = 3\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 0.0002\n",
        "\n",
        "# Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw1BKdIiF7fD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_data():\n",
        "  count = 100000\n",
        "  rand = np.random.RandomState(0)\n",
        "  a1 = [[0, 3]] + rand.randn(count // 3, 2) * 0.2\n",
        "  a2 = [[0, -3]] + rand.randn(count // 3, 2) * 0.2\n",
        "  a3 = [[3, 0]] + rand.randn(count // 3, 2) * 0.2\n",
        "  a4 = [[-3, 0]] + rand.randn(count // 3, 2) * 0.2\n",
        "  a5 = [[-2, 2.23]] + rand.randn(count // 3, 2) * 0.2\n",
        "  a6 = [[2.23, -2]] + rand.randn(count // 3, 2) * 0.2\n",
        "  a7 = [[-2.23, -2]] + rand.randn(count // 3, 2) * 0.2\n",
        "  a8 = [[2, 2.23]] + rand.randn(count // 3, 2) * 0.2\n",
        "  data_x = np.concatenate([a1, a2, a3, a4, a5, a6, a7, a8], axis=0)\n",
        "  perm = rand.permutation(len(data_x))\n",
        "  return data_x[perm]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pWvZ9OzGAOP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "792395d7-359e-47ac-da8e-7fba1a8fcfcc"
      },
      "source": [
        "data = sample_data()\n",
        "\n",
        "print(\"data shape:\",data.shape)\n",
        "print(\"data example:\", data[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data shape: (266664, 2)\n",
            "data example: [2.08119468 2.38366341]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBsFUAD2GHA_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "92152b43-4c8c-4d41-a689-666379d25fef"
      },
      "source": [
        "plt.scatter(data[:,0], data[:,1])\n",
        "plt.title(\"Datasample\")\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEICAYAAABCnX+uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncf8fHXDLBTN7I1MAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi3xcLHNG_Xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, x, transform=None):\n",
        "    self.x = torch.from_numpy(x).float()\n",
        "    self.transform = transform\n",
        "      \n",
        "  def __getitem__(self, index):\n",
        "    x = self.x[index]\n",
        "    \n",
        "    if self.transform:\n",
        "      x = self.transform(y)\n",
        "    return x\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4efPd4CdG_ax",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "49eeca14-d474-4742-f5de-01c95162d9fe"
      },
      "source": [
        "train_ds = MyDataset(data[:260000])\n",
        "len(train_ds)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "260000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGCFqBmaG_eA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVWMFtS-G_hG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generator Code\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, ngpu):\n",
        "    super(Generator, self).__init__()\n",
        "    self.ngpu = ngpu\n",
        "    self.main = nn.Sequential(\n",
        "        # input is Z, going into a convolution\n",
        "        nn.Linear(nz, 1024),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        nn.Linear(1024, 512),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(True),\n",
        "      \n",
        "        nn.Linear(256, n_out),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "    out = self.main(input)\n",
        "    # print(\"Ge out:\", out.shape)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGhaOOndHinK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "1582cd13-a64f-4788-8450-07823f3569f4"
      },
      "source": [
        "# Create the generator\n",
        "netG = Generator(ngpu).to(device)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
        "\n",
        "# Print the model\n",
        "print(netG)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): Linear(in_features=20, out_features=1024, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Linear(in_features=256, out_features=2, bias=True)\n",
            "    (7): Tanh()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxIncus4HiqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, ngpu):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.ngpu = ngpu\n",
        "    self.main = nn.Sequential(\n",
        "        # input is Z, going into a convolution\n",
        "        nn.Linear(n_out, 1024),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        nn.Linear(1024, 512),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(True),\n",
        "      \n",
        "        nn.Linear(256, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "    out = self.main(input)\n",
        "    # print(\"Ge out:\", out.shape)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGpgoO3aHitd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "1e7a9c4c-ad4f-4fec-867a-7259356db69b"
      },
      "source": [
        "# Create the Discriminator\n",
        "netD = Discriminator(ngpu).to(device)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
        "    \n",
        "\n",
        "# Print the model\n",
        "print(netD)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Linear(in_features=2, out_features=1024, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
            "    (7): Sigmoid()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Suuu-bGFHiwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize BCELoss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Create batch of latent vectors that we will use to visualize\n",
        "#  the progression of the generator\n",
        "fixed_noise = torch.randn(batch_size, nz, device=device)\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gMOtcT3KAWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_d_net_with_real(data, label):\n",
        "  '''\n",
        "  Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "  '''\n",
        "  ## Train with all-real batch\n",
        "  netD.zero_grad()\n",
        "  # Format batch\n",
        "  real_cpu = data.to(device)\n",
        "\n",
        "  # Forward pass real batch through D\n",
        "  output = netD(real_cpu).view(-1)\n",
        "  # print(\"real_out:\", output.shape)\n",
        "  # Calculate loss on all-real batch\n",
        "  errD_real = criterion(output, label)\n",
        "  # Calculate gradients for D in backward pass\n",
        "  errD_real.backward()\n",
        "  D_x = output.mean().item()\n",
        "  return errD_real, D_x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gik66nzNHi0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_d_net_with_fake(label, fake, errD_real):\n",
        "  '''\n",
        "  Train with all-fake batch\n",
        "  '''\n",
        "  \n",
        "  label.fill_(fake_label)\n",
        "  # Classify all fake batch with D\n",
        "  output = netD(fake.detach()).view(-1)\n",
        "  # Calculate D's loss on the all-fake batch\n",
        "  errD_fake = criterion(output, label)\n",
        "  # Calculate the gradients for this batch\n",
        "  errD_fake.backward()\n",
        "  D_G_z1 = output.mean().item()\n",
        "  # Add the gradients from the all-real and all-fake batches\n",
        "  errD = errD_real + errD_fake\n",
        "  # Update D\n",
        "  optimizerD.step()\n",
        "  return errD, D_G_z1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lXnLR5_J8yi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_g_net(data, label, fake):\n",
        "  '''\n",
        "  Update G network: maximize log(D(G(z)))\n",
        "  '''\n",
        "  netG.zero_grad()\n",
        "  label.fill_(real_label)  # fake labels are real for generator cost\n",
        "  # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "  output = netD(fake).view(-1)\n",
        "  # Calculate G's loss based on this output\n",
        "  errG = criterion(output, label)\n",
        "  # Calculate gradients for G\n",
        "  errG.backward()\n",
        "  D_G_z2 = output.mean().item()\n",
        "  # Update G\n",
        "  optimizerG.step()\n",
        "\n",
        "  return errG, D_G_z2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df2S3h9AJ81p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "8aabad8a-e910-417e-e308-58a873848d72"
      },
      "source": [
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  netG.train()\n",
        "  netD.train()\n",
        "  for i, x in enumerate(trainloader, 0):\n",
        "    x = x.to(device).float()\n",
        "    \n",
        "    b_size = x.size(0)\n",
        "    label = torch.full((b_size,), real_label, device=device)\n",
        "\n",
        "    errD_real, D_x = train_d_net_with_real(x, label)\n",
        "\n",
        "    # Generate batch of latent vectors\n",
        "    noise = torch.randn(batch_size, nz, device=device)\n",
        "    # Generate fake image batch with G\n",
        "    fake = netG(noise)\n",
        "\n",
        "    errD, D_G_z1 = train_d_net_with_fake(label, fake, errD_real)\n",
        "\n",
        "    errG, D_G_z2 = train_g_net(x, label, fake)\n",
        "\n",
        "\n",
        "    # print stats\n",
        "    if i % 20 == 0:\n",
        "      print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, EPOCHS, i, len(trainloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "    # Save Losses for plotting later\n",
        "    G_losses.append(errG.item())\n",
        "    D_losses.append(errD.item())\n",
        "\n",
        "    # Check how the generator is doing by saving G's output on fixed_noise\n",
        "    if (iters % 50 == 0) or ((epoch == EPOCHS-1) and (i == len(trainloader)-1)):\n",
        "      with torch.no_grad():\n",
        "        fake = netG(fixed_noise).detach().cpu()\n",
        "      img_list.append(fake)\n",
        "\n",
        "    iters += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0/3][0/260]\tLoss_D: 1.3957\tLoss_G: 0.7242\tD(x): 0.4818\tD(G(z)): 0.4859 / 0.4847\n",
            "[0/3][20/260]\tLoss_D: 0.2602\tLoss_G: 1.9205\tD(x): 0.9372\tD(G(z)): 0.1694 / 0.1465\n",
            "[0/3][40/260]\tLoss_D: 0.0444\tLoss_G: 3.5374\tD(x): 0.9883\tD(G(z)): 0.0314 / 0.0291\n",
            "[0/3][60/260]\tLoss_D: 0.0143\tLoss_G: 4.5715\tD(x): 0.9968\tD(G(z)): 0.0110 / 0.0103\n",
            "[0/3][80/260]\tLoss_D: 0.0075\tLoss_G: 5.2177\tD(x): 0.9982\tD(G(z)): 0.0057 / 0.0054\n",
            "[0/3][100/260]\tLoss_D: 0.0046\tLoss_G: 5.7287\tD(x): 0.9988\tD(G(z)): 0.0033 / 0.0033\n",
            "[0/3][120/260]\tLoss_D: 0.0031\tLoss_G: 6.1431\tD(x): 0.9991\tD(G(z)): 0.0022 / 0.0021\n",
            "[0/3][140/260]\tLoss_D: 0.0021\tLoss_G: 6.5031\tD(x): 0.9995\tD(G(z)): 0.0015 / 0.0015\n",
            "[0/3][160/260]\tLoss_D: 0.0013\tLoss_G: 6.8345\tD(x): 0.9997\tD(G(z)): 0.0011 / 0.0011\n",
            "[0/3][180/260]\tLoss_D: 0.0011\tLoss_G: 7.0884\tD(x): 0.9998\tD(G(z)): 0.0009 / 0.0008\n",
            "[0/3][200/260]\tLoss_D: 0.0009\tLoss_G: 7.3242\tD(x): 0.9998\tD(G(z)): 0.0007 / 0.0007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FNezWpqJ85D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWM5Tv2lJ876",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%capture\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "ims = [[plt.scatter(i[:,0], i[:,1])] for i in img_list]\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCUxLZMDQ8Bn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(img_list))\n",
        "print(img_list[1].shape)\n",
        "print(img_list[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3efWmgZTQ0cM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}